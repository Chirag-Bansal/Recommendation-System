# -*- coding: utf-8 -*-
"""Recommendation System.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OW-5-fUG_x0SGabSRcoUR2eG80WlhmAu
"""

from google.colab import drive
drive.mount("/content/gdrive")

import pandas as pd
from collections import defaultdict
import numpy as np

"""## Loading dataset

Converted the excel file to a pandas dataframe
"""

path = "/content/gdrive/MyDrive/Recommender.xlsx"

df = pd.io.excel.read_excel(path)

df.head()

"""##Seperation of the dataset
We seprate the dataset into 2 parts - one with year 2019/2020 and the other with year 2021. The former would be called training data and the latter validation data. To check our model we seperate a validation set on which we will testing our dataset, we use the Leave one out cross validation(LOOCV) method and we pick the user randomly.
"""

year_list = ['2021']
valid_df = df[df.Year.isin(year_list)]
year_list = ['2019','2020']
train_df = df[df.Year.isin(year_list)]

train_df.head()

valid_df.head()

"""We find the set of users and the set of items on the whole dataset"""

userSet = set()
itemSet = set()
for index,d in df.iterrows():
  userSet.add(d['UserId'])
  itemSet.add(d['ProductId'])

total_users = len(userSet)
total_items = len(itemSet)

import random

ValidUserId = random.randint(1,total_users)
Valid_User_item_matrix = user_item_matrix[ValidUserId]

"""##User and Item datasets

For a recommendation system to work properly, we would require three types of data -


1.   **User Dictionary:** These contain the items bought by each user
2.   **Item Dictionary:** These contain the users that bought each item.
3. **User-Item Matrix:** This contains the items each user bought as well as how many times that particular item was bought.


"""

usersPerItem = defaultdict(set)
itemsPerUser = defaultdict(set)

for index,d in train_df.iterrows():
    user,item = d['UserId'], d['ProductId']
    usersPerItem[item].add(user)
    itemsPerUser[user].add(item)

#The above only contains the set of user/items of training dataset
totalUsersPerItem = defaultdict(set)
totalItemsPerUser = defaultdict(set)
for index,d in df.iterrows():
    user,item = d['UserId'], d['ProductId']
    totalUsersPerItem[item].add(user)
    totalItemsPerUser[user].add(item)

"""We have created the sets of users per item and items per user"""

train_items = len(usersPerItem)
print("The total number of items in the training dataset are", train_items)
train_users = len(itemsPerUser)
print("The total number of users in the training dataset are", train_users)
print("The items bought by user with UserID 1 in 2019/20 is - ", itemsPerUser[1])

user_item_matrix = np.empty([total_users, total_items])

for index,d in train_df.iterrows():
    user,item = d['UserId'], d['ProductId']
    user_item_matrix[int(user) - 1][int(item) - 1] += 1

"""We have created a user-item matrix"""

print("The items bought by user with UserID 1 in 2019/20 is - ")
for i in range(total_items):
  print("The product with ProductID" , i+1 , "was bought" , int(user_item_matrix[0][i]) , "times")

"""As you can see this also encodes the information about which product was bought how many times.

##Jaccard Index
The Jaccard index is a statistic used for gauging the similarity and diversity of sample sets. The Jaccard coefficient measures similarity between finite sample sets, and is defined as the size of the intersection divided by the size of the union of the sample sets.

The thing to note is that Jaccard Similarity finds the similarity between two
"""

def Jaccard(s1, s2):
  if(len(s1) == 0 or len(s2) == 0):
    return 0
  numer = len(s1.intersection(s2))
  denom = len(s1.union(s2))
  return numer / denom

"""Based on the Jaccard similarity, we can create a similarity matrix between all the items."""

jaccard_sim_matrix = np.zeros([total_items, total_items])

for i in range(total_items):
  for j in range(total_items):
    set_one = usersPerItem[i+1]
    set_two = usersPerItem[j+1]
    jaccard_index = Jaccard(set_one, set_two)
    jaccard_sim_matrix[i][j] = jaccard_index

"""The following color plot, shows the similarity between items."""

import matplotlib.pyplot as plt

plt.imshow(jaccard_sim_matrix)
plt.colorbar()
plt.show()

"""For each item we can see all the similar items, and thus can use this for recommendation, we also calculate the same for each user."""

jaccard_sim_matrix_users = np.zeros([total_users, total_users])

for i in range(total_users):
  for j in range(total_users):
    set_one = itemsPerUser[i+1]
    set_two = itemsPerUser[j+1]
    jaccard_index = Jaccard(set_one, set_two)
    jaccard_sim_matrix_users[i][j] = jaccard_index

"""The following color plot, shows the similarity between users."""

import matplotlib.pyplot as plt

plt.imshow(jaccard_sim_matrix_users)
plt.colorbar()
plt.show()

"""##Cosine Similarity
Given that we have a vector of items for each user we can find similarity between them.
"""

#This checks whether the array is all zeros
def check(arr):
    if np.all(arr == 0):
        return True
    return False

from scipy import spatial

def cosine_sim(a, b):
  if(check(a) or check(b)):
    return 0.0
  cosine_similarity = 1 - float(spatial.distance.cosine(a, b))
  return cosine_similarity

"""To find the Item-Item cosine similarity we take the column of the user-item matrix and fine their similarities"""

Item_vector_one = user_item_matrix[:,0]
Item_vector_two = user_item_matrix[:,1]

print("Cosine Similarity: ", cosine_sim(Item_vector_one,Item_vector_two))

cosine_similarity_matrix = np.zeros([total_items, total_items])

for i in range(total_items):
  for j in range(total_items):
    Item_vector_one = user_item_matrix[:,i]
    Item_vector_two = user_item_matrix[:,j]
    cos_sim = cosine_sim(Item_vector_one,Item_vector_two)
    cosine_similarity_matrix[i][j] = cos_sim

"""The following shows the similarity measure in items-

"""

plt.imshow(cosine_similarity_matrix)
plt.colorbar()
plt.show()

"""##Item Based Collaborative Filtering

Now that we have a similarity measure between the items we can use this to recommend items to users

##**Model 1:** K-Nearest Neighbours with cosine similarity

The algorithm works as follows-


*   For each user determine the set of items bought
*   For each item in the list find the K-Nearest items based on cosine similarity
*   From the set we multiply the cosine similarity based on how many times the user has bought that item(the one that was used to find the similar item list)
*   We sort the items based on the value.
*   We can calculate recall and precision for the user.
*   We make predictions based on the items.
"""

def getTopKforItem(ProductId,k):
  cosine_vector = cosine_similarity_matrix[ProductId-1]
  a = dict([(i+1, j) for i, j in enumerate(cosine_vector)])
  sorted_a = dict(sorted(a.items(), key = lambda kv:kv[1], reverse=True))
  indices = list(sorted_a.keys())[1:k+1]
  values = list(sorted_a.values())[1:k+1]
  return (indices,values)

print(getTopKforItem(1,3))

"""The above code finds the nearest items for a particular item"""

def getTopKforUser(UserID,k):
  items = totalItemsPerUser[UserID]
  recommendations = {}
  for item in items:
    item_reccomendations = getTopKforItem(item,k)
    num = user_item_matrix[UserID-1][item-1]
    for i in range(k):
      item_rec = item_reccomendations[0][i]
      item_rec_val = num*item_reccomendations[1][i]
      if item_rec in recommendations:
        recommendations[item_rec] = recommendations[item_rec]+item_rec_val
      else:
        recommendations[item_rec] = item_rec_val
  sorted_rec = dict(sorted(recommendations.items(), key = lambda kv:kv[1], reverse=True))
  indices = list(sorted_rec.keys())[:k]
  values = list(sorted_rec.values())[:k]
  return (indices,values)

"""The above code takes a user and finds the products that should be recommended based on """

print(itemsPerUser[1])

print(getTopKforUser(1,1))

precision = np.zeros([total_items-1])
recall = np.zeros([total_items-1])
times = 0
for user in range(1,total_users):
  items = totalItemsPerUser[user]
  for k in range(1,total_items):
    recs = getTopKforUser(user,k)[0]
    pres = 0
    rec = 0
    for item in recs:
      if item in items:
        rec += 1
        pres += 1
    try:
      pres /= len(recs)
      rec /= len(items)
    except:
      pres = 0
      rec = 0
    precision[k-1] += pres
    recall[k-1] += rec
  times +=1.0
precision = np.divide(precision, times)
recall = np.divide(recall, times)
print(precision)
print(recall)

"""##Precision and Recall curves
We are getting k nearest items for each user, we vary this k from 1 to total number of items which is the max we can get. We plot the precision and recall curves for each value of K.
"""

plt.plot(precision[:])
plt.xlabel("Number of recommendations")
plt.ylabel("Precision")
plt.show()

plt.plot(recall[:])
plt.xlabel("Number of recommendations")
plt.ylabel("Recall")
plt.show()

"""Note that the precision will continue to decrease after some points as we are recommending more than what the user has bought. Whereas the recall should keep on increasing till all the relevant prodcuts are shown."""

rec_data = []
for users in range(1,total_users):
  recs = getTopKforUser(users,3)[0]
  recs.insert(0,users)
  rec_data.append(recs)

df = pd.DataFrame(rec_data, columns = ['UserId', 'Recommendation 1','Recommendation 2','Recommendation 3'])

df.to_csv('./knn.csv', sep = ',', index = False)

from google.colab import files
files.download('knn.csv')

"""###Predictions on Validation set
Based on the KNN model formed above we test our results on the validation dataset
"""

precision = 0
recall = 0
times = 0

for index,d in valid_df.iterrows():
  ValidUserId = d['UserId']
  items = totalItemsPerUser[ValidUserId]
  recs = getTopKforUser(user,3)[0]
  pres = 0
  rec = 0
  for item in recs:
    if item in items:
      rec += 1
      pres += 1
  pres /= len(recs)
  rec /= len(items)
  precision += pres
  recall += rec
  times += 1

precision = np.divide(precision, times)
recall = np.divide(recall, times)

print("The precision on the validation dataset is" , precision)
print("The recall on the validation dataset is" , recall)

"""##**Model 2:** Jaccard Index

The algorithm works as follows-


*   For each user determine the set of items bought
*   For each item in the list find the items based on jaccard index
*   We sort the items based on the value.
*   We can calculate recall and precision for the user.
*   We make predictions based on the items.


"""

def getTopKforItemJI(ProductId,k):
  cosine_vector = jaccard_sim_matrix[ProductId-1]
  a = dict([(i+1, j) for i, j in enumerate(cosine_vector)])
  sorted_a = dict(sorted(a.items(), key = lambda kv:kv[1], reverse=True))
  indices = list(sorted_a.keys())[1:k+1]
  values = list(sorted_a.values())[1:k+1]
  return (indices,values)

print(getTopKforItemJI(1,3))

"""The above code finds the nearest items for a particular item"""

def getTopKforUserJI(UserID,k):
  items = totalItemsPerUser[UserID]
  recommendations = {}
  for item in items:
    item_reccomendations = getTopKforItemJI(item,k)
    for i in range(k):
      item_rec = item_reccomendations[0][i]
      item_rec_val = item_reccomendations[1][i]
      if item_rec in recommendations:
        recommendations[item_rec] = max(recommendations[item_rec],item_rec_val)
      else:
        recommendations[item_rec] = item_rec_val
  sorted_rec = dict(sorted(recommendations.items(), key = lambda kv:kv[1], reverse=True))
  indices = list(sorted_rec.keys())[:k]
  values = list(sorted_rec.values())[:k]
  return (indices,values)

"""The above code takes a user and finds the products that should be recommended based on """

print(itemsPerUser[1])

print(getTopKforUserJI(1,1))

precisionJI = np.zeros([total_items-1])
recallJI = np.zeros([total_items-1])
times = 0
for user in range(1,total_users):
  items = totalItemsPerUser[user]
  for k in range(1,total_items):
    recs = getTopKforUserJI(user,k)[0]
    pres = 0
    rec = 0
    for item in recs:
      if item in items:
        rec += 1
        pres += 1
    try:
      pres /= len(recs)
      rec /= len(items)
    except:
      pres = 0
      rec = 0
    precisionJI[k-1] += pres
    recallJI[k-1] += rec
  times +=1.0
precisionJI = np.divide(precisionJI, times)
recallJI = np.divide(recallJI, times)
print(precisionJI)
print(recallJI)

"""##Precision and Recall curves
We are getting k nearest items for each user, we vary this k from 1 to total number of items which is the max we can get. We plot the precision and recall curves for each value of K.
"""

plt.plot(precisionJI[:])
plt.xlabel("Number of recommendations")
plt.ylabel("Precision")
plt.show()

plt.plot(recallJI[:])
plt.xlabel("Number of recommendations")
plt.ylabel("Recall")
plt.show()

"""Note that the precision will continue to decrease after some points as we are recommending more than what the user has bought. Whereas the recall should keep on increasing till all the relevant prodcuts are shown."""

rec_dataJI = []
for users in range(1,total_users):
  recs = getTopKforUserJI(users,3)[0]
  recs.insert(0,users)
  rec_data.append(recs)

dfJI = pd.DataFrame(rec_data, columns = ['UserId', 'Recommendation 1','Recommendation 2','Recommendation 3'])

dfJI.to_csv('./jaccard.csv', sep = ',', index = False)

from google.colab import files
files.download('jaccard.csv')

"""###Predictions on Validation set-"""

precision_jaccard = 0
recall_jaccard = 0
times = 0

for index,d in valid_df.iterrows():
  ValidUserId = d['UserId']
  items = totalItemsPerUser[ValidUserId]
  recs = getTopKforUserJI(user,3)[0]
  pres = 0
  rec = 0
  for item in recs:
    if item in items:
      rec += 1
      pres += 1
  pres /= len(recs)
  rec /= len(items)
  precision_jaccard += pres
  recall_jaccard += rec
  times += 1

precision_jaccard /= times
recall_jaccard /= times

print("The precision on the validation dataset is" , precision_jaccard)
print("The recall on the validation dataset is" , recall_jaccard)

"""##Jaccard Index v/s K-Nearest Neighbours
**The values for Jaccard Index are -** 
* The precision on the validation dataset is 0.556848701880038
* The recall on the validation dataset is 0.36793436345808683

**The values for K-Nearest Neighbours are -** 
* The precision on the validation dataset is 0.6869591166815853
* The recall on the validation dataset is 0.44883894579507744

It is evident from the values that the K-nearest Neighbours performs better than the Jaccard Index when testing on the validation dataset
"""